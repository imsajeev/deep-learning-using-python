{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluating Models",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3fg_acm9UzR",
        "colab_type": "text"
      },
      "source": [
        "# Evaluating Models\n",
        "Evaluating models is the process of understanding how well they give the correct classification and then measuring the value of the prediction in a certain context.\n",
        "\n",
        "Good accuracy (not only enough)\n",
        "  Eg: Disease Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j_C9RR_9swn",
        "colab_type": "text"
      },
      "source": [
        "## Confusion Matrix\n",
        "![Confusion Matrix](https://2.bp.blogspot.com/-EvSXDotTOwc/XMfeOGZ-CVI/AAAAAAAAEiE/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs/s1600/confusionMatrxiUpdated.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_sH9402-Ymi",
        "colab_type": "text"
      },
      "source": [
        "**1.     Positive prediction**\n",
        "*   Positive prediction\n",
        "*   Label was positive\n",
        "\n",
        "**2.   Label was positive**\n",
        "\n",
        "*   Positive prediction\n",
        "*   Label was negative\n",
        "\n",
        "**3. True negatives**\n",
        "\n",
        "*   Negative prediction\n",
        "*   Label was negative\n",
        "\n",
        "**4. False negatives**\n",
        "\n",
        "\n",
        "*   Negative prediction\n",
        "*   Label was positive\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqqkAFd1AFj7",
        "colab_type": "text"
      },
      "source": [
        "## Sensitivity versus specificity\n",
        "Sensitivity - The true positive rate measures how often we classify an input record as\n",
        "the positive class and its the correct classification.\n",
        "\n",
        "*Example would be classifying a patient as having a condition who was actually sick. Sensitivity quantifies how well the model avoids false negatives*\n",
        "\n",
        "**Sensitivity = TP / (TP + FN)**\n",
        "\n",
        "Specificity quantifies how well the model avoids false positives\n",
        "\n",
        "**Specificity = TN / (TN + FP)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6fz6-rsDa4G",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy\n",
        "Accuracy is the degree of closeness of measurements of a quantity to that\n",
        "quantity’s true value.\n",
        "\n",
        "**Accuracy = (TP + TN) / (TP + FP + FN + TN)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im1iQQ2iDlLe",
        "colab_type": "text"
      },
      "source": [
        "## Precision\n",
        "The degree to which repeated measurements under the same conditions give us\n",
        "the same results is called precision\n",
        "\n",
        "\n",
        "Also known as the positive prediction value\n",
        "\n",
        "\n",
        "**Precision = TP / (TP + FP)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Kw_KY2nDv1k",
        "colab_type": "text"
      },
      "source": [
        "## Recall\n",
        "Same as Sensitivity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e69Aw5FEDxV",
        "colab_type": "text"
      },
      "source": [
        "## F1\n",
        "In binary classification we consider the F1 score to be a measure of a model’s accuracy.\n",
        "\n",
        "F1 = 2TP / (2TP + FP + FN)\n",
        "\n",
        "We see scores for F1 between 0.0 and 1.0, where 0.0 is the worst score and 1.0 is the best score we’d like to see."
      ]
    }
  ]
}