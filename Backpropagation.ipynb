{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Backpropagation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0RLn_MCVedO",
        "colab_type": "text"
      },
      "source": [
        "# Initialize a network\n",
        "\n",
        "Below is a function named initialize_network() that creates a new neural network ready for training. It accepts three parameters, the number of inputs, the number of neurons to have in the hidden layer and the number of outputs.\n",
        "\n",
        "You can see that for the hidden layer we create n_hidden neurons and each neuron in the hidden layer has n_inputs + 1 weights, one for each input column in a dataset and an additional one for the bias.\n",
        "\n",
        "You can also see that the output layer that connects to the hidden layer has n_outputs neurons, each with n_hidden + 1 weights. This means that each neuron in the output layer connects to (has a weight for) each neuron in the hidden layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34Zz4N3cgtY5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "935bd900-3094-46dd-a9a6-7d608ed53471"
      },
      "source": [
        "from random import seed\n",
        "from random import random\n",
        "\n",
        "# Initialize a network\n",
        "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
        "\tnetwork = list()\n",
        "\thidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
        "\tnetwork.append(hidden_layer)\n",
        "\toutput_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
        "\tnetwork.append(output_layer)\n",
        "\treturn network\n",
        "\n",
        "seed(1)\n",
        "network = initialize_network(2, 1, 2)\n",
        "for layer in network:\n",
        "\tprint(layer)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n",
            "[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IOYk7AGVxYL",
        "colab_type": "text"
      },
      "source": [
        "## Neuron Activation\n",
        "The first step is to calculate the activation of one neuron given an input.\n",
        "\n",
        "The input could be a row from our training dataset, as in the case of the hidden layer. It may also be the outputs from each neuron in the hidden layer, in the case of the output layer.\n",
        "\n",
        "Neuron activation is calculated as the weighted sum of the inputs. Much like linear regression.\n",
        "\n",
        "**activation = sum(weight_i * input_i) + bias**\n",
        "\n",
        "\n",
        "\n",
        "Where weight is a network weight, input is an input, i is the index of a weight or an input and bias is a special weight that has no input to multiply with (or you can think of the input as always being 1.0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzKxa0aihAUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate neuron activation for an input\n",
        "def activate(weights, inputs):\n",
        "\tactivation = weights[-1]\n",
        "\tfor i in range(len(weights)-1):\n",
        "\t\tactivation += weights[i] * inputs[i]\n",
        "\treturn activation"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c_ECd7OWMXE",
        "colab_type": "text"
      },
      "source": [
        "## Neuron Transfer\n",
        "Once a neuron is activated, we need to transfer the activation to see what the neuron output actually is.\n",
        "\n",
        "Different transfer functions can be used. It is traditional to use the sigmoid activation function, but you can also use the tanh (hyperbolic tangent) function to transfer outputs. More recently, the rectifier transfer function has been popular with large deep learning networks.\n",
        "\n",
        "The sigmoid activation function looks like an S shape, it’s also called the logistic function. It can take any input value and produce a number between 0 and 1 on an S-curve. It is also a function of which we can easily calculate the derivative (slope) that we will need later when backpropagating error.\n",
        "\n",
        "We can transfer an activation function using the sigmoid function as follows:\n",
        "\n",
        "**output = 1 / (1 + e^(-activation))**\n",
        "\n",
        "\n",
        "Where e is the base of the natural logarithms (Euler’s number)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM__UPSxhA9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transfer neuron activation\n",
        "def transfer(activation):\n",
        "\treturn 1.0 / (1.0 + exp(-activation))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0phrc-qWZXI",
        "colab_type": "text"
      },
      "source": [
        "## Forward Propagation\n",
        "Forward propagating an input is straightforward.\n",
        "\n",
        "We work through each layer of our network calculating the outputs for each neuron. All of the outputs from one layer become inputs to the neurons on the next layer.\n",
        "\n",
        "Below is a function named forward_propagate() that implements the forward propagation for a row of data from our dataset with our neural network.\n",
        "\n",
        "You can see that a neuron’s output value is stored in the neuron with the name ‘output‘. You can also see that we collect the outputs for a layer in an array named new_inputs that becomes the array inputs and is used as inputs for the following layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ic8SPrWhTZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Forward propagate input to a network output\n",
        "def forward_propagate(network, row):\n",
        "\tinputs = row\n",
        "\tfor layer in network:\n",
        "\t\tnew_inputs = []\n",
        "\t\tfor neuron in layer:\n",
        "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
        "\t\t\tneuron['output'] = transfer(activation)\n",
        "\t\t\tnew_inputs.append(neuron['output'])\n",
        "\t\tinputs = new_inputs\n",
        "\treturn inputs"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsoftW8hWfKN",
        "colab_type": "text"
      },
      "source": [
        "## all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5V2BpRu7hWrk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f84a339c-a44d-417d-f956-865c206ec37b"
      },
      "source": [
        "from math import exp\n",
        "\n",
        "# Calculate neuron activation for an input\n",
        "def activate(weights, inputs):\n",
        "\tactivation = weights[-1]\n",
        "\tfor i in range(len(weights)-1):\n",
        "\t\tactivation += weights[i] * inputs[i]\n",
        "\treturn activation\n",
        "\n",
        "# Transfer neuron activation\n",
        "def transfer(activation):\n",
        "\treturn 1.0 / (1.0 + exp(-activation))\n",
        "\n",
        "# Forward propagate input to a network output\n",
        "def forward_propagate(network, row):\n",
        "\tinputs = row\n",
        "\tfor layer in network:\n",
        "\t\tnew_inputs = []\n",
        "\t\tfor neuron in layer:\n",
        "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
        "\t\t\tneuron['output'] = transfer(activation)\n",
        "\t\t\tnew_inputs.append(neuron['output'])\n",
        "\t\tinputs = new_inputs\n",
        "\treturn inputs\n",
        "\n",
        "# test forward propagation\n",
        "network = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}],\n",
        "\t\t[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
        "row = [1, 0, None]\n",
        "output = forward_propagate(network, row)\n",
        "print(output)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6629970129852887, 0.7253160725279748]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDYFfgF9WumN",
        "colab_type": "text"
      },
      "source": [
        "# Back Propagate Error\n",
        "## Transfer Derivative\n",
        "Given an output value from a neuron, we need to calculate it’s slope.\n",
        "\n",
        "We are using the sigmoid transfer function, the derivative of which can be calculated as follows:\n",
        "\n",
        "**derivative = output * (1.0 - output)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1FrDQZpheTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the derivative of an neuron output\n",
        "def transfer_derivative(output):\n",
        "\treturn output * (1.0 - output)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QMPcLn8XBlx",
        "colab_type": "text"
      },
      "source": [
        "## Error Backpropagation\n",
        "The first step is to calculate the error for each output neuron, this will give us our error signal (input) to propagate backwards through the network.\n",
        "\n",
        "The error for a given neuron can be calculated as follows:\n",
        "\n",
        "**error = (expected - output) * transfer_derivative(output)**\n",
        "\n",
        "Where expected is the expected output value for the neuron, output is the output value for the neuron and transfer_derivative() calculates the slope of the neuron’s output value, as shown above.\n",
        "\n",
        "This error calculation is used for neurons in the output layer. The expected value is the class value itself. In the hidden layer, things are a little more complicated.\n",
        "\n",
        "The error signal for a neuron in the hidden layer is calculated as the weighted error of each neuron in the output layer. Think of the error traveling back along the weights of the output layer to the neurons in the hidden layer.\n",
        "\n",
        "The back-propagated error signal is accumulated and then used to determine the error for the neuron in the hidden layer, as follows:\n",
        "\n",
        "**error = (weight_k * error_j) * transfer_derivative(output)**\n",
        "\n",
        "Where error_j is the error signal from the jth neuron in the output layer, weight_k is the weight that connects the kth neuron to the current neuron and output is the output for the current neuron."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of0dyTIHhl5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Backpropagate error and store in neurons\n",
        "def backward_propagate_error(network, expected):\n",
        "\tfor i in reversed(range(len(network))):\n",
        "\t\tlayer = network[i]\n",
        "\t\terrors = list()\n",
        "\t\tif i != len(network)-1:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\terror = 0.0\n",
        "\t\t\t\tfor neuron in network[i + 1]:\n",
        "\t\t\t\t\terror += (neuron['weights'][j] * neuron['delta'])\n",
        "\t\t\t\terrors.append(error)\n",
        "\t\telse:\n",
        "\t\t\tfor j in range(len(layer)):\n",
        "\t\t\t\tneuron = layer[j]\n",
        "\t\t\t\terrors.append(expected[j] - neuron['output'])\n",
        "\t\tfor j in range(len(layer)):\n",
        "\t\t\tneuron = layer[j]\n",
        "\t\t\tneuron['delta'] = errors[j] * transfer_derivative(neuron['output'])"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1jf6E2AXQn-",
        "colab_type": "text"
      },
      "source": [
        "#Train Network\n",
        "## Update Weights\n",
        "Once errors are calculated for each neuron in the network via the back propagation method above, they can be used to update weights.\n",
        "\n",
        "Network weights are updated as follows:\n",
        "\n",
        "**weight = weight + learning_rate * error * input**\n",
        "\n",
        "Where weight is a given weight, learning_rate is a parameter that you must specify, error is the error calculated by the backpropagation procedure for the neuron and input is the input value that caused the error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJK05h97h0q1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Update network weights with error\n",
        "def update_weights(network, row, l_rate):\n",
        "\tfor i in range(len(network)):\n",
        "\t\tinputs = row[:-1]\n",
        "\t\tif i != 0:\n",
        "\t\t\tinputs = [neuron['output'] for neuron in network[i - 1]]\n",
        "\t\tfor neuron in network[i]:\n",
        "\t\t\tfor j in range(len(inputs)):\n",
        "\t\t\t\tneuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
        "\t\t\tneuron['weights'][-1] += l_rate * neuron['delta']"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUwQ_wRnXb4b",
        "colab_type": "text"
      },
      "source": [
        "## Train Network\n",
        "As mentioned, the network is updated using stochastic gradient descent.\n",
        "\n",
        "This involves first looping for a fixed number of epochs and within each epoch updating the network for each row in the training dataset.\n",
        "\n",
        "Because updates are made for each training pattern, this type of learning is called online learning. If errors were accumulated across an epoch before updating the weights, this is called batch learning or batch gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BqIpE0Lh6NR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train a network for a fixed number of epochs\n",
        "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
        "  for epoch in range(n_epoch):\n",
        "    sum_error = 0\n",
        "    for row in train:\n",
        "\t\t    outputs = forward_propagate(network, row)\n",
        "\t\t    expected = [0 for i in range(n_outputs)]\n",
        "\t\t    expected[row[-1]] = 1\n",
        "\t\t    sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
        "\t\t    backward_propagate_error(network, expected)\n",
        "\t\t    update_weights(network, row, l_rate)\n",
        "    \n",
        "    print ('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drKCIHLcXjzX",
        "colab_type": "text"
      },
      "source": [
        "# Test training backprop algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpsn9zHCh-hv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test training backprop algorithm\n",
        "seed(1)\n",
        "dataset = [\n",
        "  [2.7810836,2.550537003,0],\n",
        "\t[1.465489372,2.362125076,0],\n",
        "\t[3.396561688,4.400293529,0],\n",
        "\t[1.38807019,1.850220317,0],\n",
        "\t[3.06407232,3.005305973,0],\n",
        "\t[7.627531214,2.759262235,1],\n",
        "\t[5.332441248,2.088626775,1],\n",
        "\t[6.922596716,1.77106367,1],\n",
        "\t[8.675418651,-0.242068655,1],\n",
        "\t[7.673756466,3.508563011,1]]\n",
        "n_inputs = len(dataset[0]) - 1\n",
        "n_outputs = len(set([row[-1] for row in dataset]))\n",
        "network = initialize_network(n_inputs, 2, n_outputs)\n",
        "train_network(network, dataset, 0.5, 20, n_outputs)\n",
        "for layer in network:\n",
        "\tprint(layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg4HxfAvXrY0",
        "colab_type": "text"
      },
      "source": [
        "# Make a prediction with a network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieL7IzoRklJm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "bb1c0cc1-35c0-4fd5-d096-0e82b5e2045b"
      },
      "source": [
        "# Make a prediction with a network\n",
        "def predict(network, row):\n",
        "\toutputs = forward_propagate(network, row)\n",
        "\treturn outputs.index(max(outputs))\n",
        "for row in dataset:\n",
        "\tprediction = predict(network, row)\n",
        "\tprint('Expected=%d, Got=%d' % (row[-1], prediction))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=0, Got=0\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n",
            "Expected=1, Got=1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}