{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Loss Function.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fghNtxZH0ICU",
        "colab_type": "text"
      },
      "source": [
        "# Mean squared error loss (MSE)\n",
        "The Mean Squared Error (MSE) or Mean Squared Deviation (MSD) of an estimator measures the average of error squares i.e. the average squared difference between the estimated values and true value. It is a risk function, corresponding to the expected value of the squared error loss. \n",
        "\n",
        "**It is always non – negative and values close to zero are better.** \n",
        "\n",
        "The equation of the function is \n",
        "\n",
        "\n",
        "![MSE](https://miro.medium.com/max/808/1*-e1QGatrODWpJkEwqP4Jyg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HylQBDfPvO-X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7b20ba37-b339-459d-c3d1-922a90766bed"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error \n",
        "\n",
        "# Given values \n",
        "Y_true = [1,1,2,2,4] # Y_true = Y (original values) \n",
        "\n",
        "# calculated values \n",
        "Y_pred = [0.6,1.29,1.99,2.69,3.4] # Y_pred = Y' \n",
        "\n",
        "# Calculation of Mean Squared Error (MSE) \n",
        "mean_squared_error(Y_true,Y_pred) \n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.21606"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPnvIXtW06rO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7302ab3f-6731-4503-98be-4a539140b729"
      },
      "source": [
        "#Manual Calculation of the MSE for above code\n",
        "(1/5)*((-0.4*-0.4)+(0.29*0.29)+(-0.01*-0.01)+(0.69*0.69)+(-0.6*-0.6))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.21605999999999997"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAZKgmUD3FW3",
        "colab_type": "text"
      },
      "source": [
        "# Mean Absolute error (MAE)\n",
        "\n",
        "It is the average of sum of absolute differences between predictions and actual observations\n",
        "\n",
        "The equation of the function is \n",
        "\n",
        "![alt text](https://miro.medium.com/max/780/1*fYNhlncTwLYqUl-_H6YElA.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egOPnaG32WnG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "43fc3ac3-a598-4a60-9eda-dd9ef1b12a9c"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error \n",
        "\n",
        "# Given values \n",
        "Y_true = [1,1,2,2,4] # Y_true = Y (original values) \n",
        "\n",
        "# calculated values \n",
        "Y_pred = [0.6,1.29,1.99,2.69,3.4] # Y_pred = Y' \n",
        "\n",
        "# Calculation of mean_absolute_error (MSE) \n",
        "mean_absolute_error(Y_true,Y_pred) \n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.398"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9tExL1K2u8W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "352ed765-20b6-49a9-c52d-9f181410987f"
      },
      "source": [
        "#Manual Calculation\n",
        "(1/5)*(0.4+0.29+0.01+0.69+0.6)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.39799999999999996"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lPVqBVc4svq",
        "colab_type": "text"
      },
      "source": [
        "# Mean squared logarithmic error (MSLE)\n",
        "Use MSLE when doing regression, believing that your target, conditioned on the input, is normally distributed, and you don’t want large errors to be significantly more penalized than small ones, in those cases where the range of the target value is large.\n",
        "\n",
        "Example: You want to predict future house prices, and your dataset includes homes that are orders of magnitude different in price. The price is a continuous value, and therefore, we want to do regression. MSLE can here be used as the loss function.\n",
        "\n",
        "The expression is as follows\n",
        "![alt text](https://raw.githubusercontent.com/imsajeev/deep-learning-using-python/master/MSLE.jpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7BgbDoK6Hgy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1ca92008-e4f1-41b5-8861-24fd1c70fb6b"
      },
      "source": [
        "from sklearn.metrics import mean_squared_log_error \n",
        "\n",
        "# Given values \n",
        "Y_true = [1,1,2,2,4] # Y_true = Y (original values) \n",
        "\n",
        "# calculated values \n",
        "Y_pred = [0.6,1.29,1.99,2.69,3.4] # Y_pred = Y' \n",
        "\n",
        "# Calculation of mean_squared_log_error  \n",
        "mean_squared_log_error(Y_true,Y_pred) \n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.025466969135005298"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwQ2bCZc-JCY",
        "colab_type": "text"
      },
      "source": [
        "# Mean absolute percentage error (MAPE)\n",
        "The mean absolute percentage error (MAPE), also known as mean absolute percentage deviation (MAPD), is a measure of prediction accuracy of a forecasting method in statistics, for example in trend estimation, also used as a loss function for regression problems in machine learning. It usually expresses the accuracy as a ratio defined by the formula:\n",
        "\n",
        "![alt text](https://i.imgur.com/OBBvmIH.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzntBpPg_M37",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "74bd45e7-f032-49f8-f532-61a47e4f50a4"
      },
      "source": [
        "#from sklearn.metrics import mean_absolute_percentage_error\n",
        "import numpy as np\n",
        "\n",
        "# Given values \n",
        "Y_true = [1,1,2,2,4] # Y_true = Y (original values) \n",
        "\n",
        "# calculated values \n",
        "Y_pred = [0.6,1.29,1.99,2.69,3.4] # Y_pred = Y' \n",
        "\n",
        "# Calculation of mean_squared_log_error  \n",
        "#mean_absolute_percentage_error(Y_true,Y_pred) \n",
        "def mean_absolute_percentage_error(y_true, y_pred): \n",
        "  y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "  mape=(np.mean(np.abs((y_true - y_pred) / y_true)))*(100/len(Y_true))\n",
        "  return mape\n",
        "\n",
        "\n",
        "MAPE=mean_absolute_percentage_error(Y_true,Y_pred)\n",
        "MAPE"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.76"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K7GUae3DRJ0",
        "colab_type": "text"
      },
      "source": [
        "# Classification Loss\n",
        "##1.Hinge Loss\n",
        "also known as Multi class SVM Loss. Hinge loss is applied for maximum-margin classification, prominently for support vector machines. It is a convex function used in convex optimizers.\n",
        "\n",
        "Hinge loss is the most commonly used loss function when the network must be\n",
        "optimized for a hard classification. For example, 0 = no fraud and 1 = fraud,\n",
        "which by convention is called a 0-1 classifier. The 0,1 choice is somewhat\n",
        "arbitrary and –1, 1 is also seen in lieu of 0–1. Hinge loss is also seen in a class of\n",
        "models called maximum-margin classification models (e.g., support vector\n",
        "machines, a somewhat distant cousin to neural networks).\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/imsajeev/deep-learning-using-python/master/hingeloss.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqKAjZoZF-Ya",
        "colab_type": "text"
      },
      "source": [
        "##Hinge Loss for two case prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmnIj3njESKO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "383dc8f8-6115-43c9-c63c-b9ebbc0d2a23"
      },
      "source": [
        "#Create a Sample SVM Model\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import hinge_loss\n",
        "X = [[0], [1],[2],[-5]]\n",
        "y = [-1, 1,1,-1]\n",
        "est = svm.LinearSVC(random_state=0)\n",
        "est.fit(X, y)\n",
        "#LinearSVC(random_state=0)\n",
        "pred_decision = est.decision_function([[-2], [3], [0.5]])\n",
        "pred_decision\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.18181059,  2.36363311,  0.09091126])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcyOhGAiGGjl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "147afdf7-ef75-4f47-a80b-805f28fea9d7"
      },
      "source": [
        "#finding Hinge loss\n",
        "hinge_loss([-1, 1, 1], pred_decision)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3030295801509197"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCjk0nEXGli4",
        "colab_type": "text"
      },
      "source": [
        "##Hinge Loss for Multi case prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHWLR9QrGoUv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d36aed6a-bf49-4229-a070-90cf777b244a"
      },
      "source": [
        "import numpy as np\n",
        "X = np.array([[0], [1], [2], [3]])\n",
        "Y = np.array([0, 1, 2, 3])\n",
        "labels = np.array([0, 1, 2, 3])\n",
        "est = svm.LinearSVC()\n",
        "est.fit(X, Y)\n",
        "#LinearSVC()\n",
        "pred_decision = est.decision_function([[-1], [2], [3]])\n",
        "y_true = [0, 2, 3]\n",
        "hinge_loss(y_true, pred_decision, labels=labels)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5641164913813278"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTvhwuhqG1-e",
        "colab_type": "text"
      },
      "source": [
        "## 2.Logistic loss\n",
        "\n",
        "###Introduction\n",
        "Log Loss is the most important classification metric based on probabilities.\n",
        "\n",
        "It's hard to interpret raw log-loss values, but log-loss is still a good metric for comparing models. For any given problem, a lower log-loss value means better predictions.\n",
        "\n",
        "Log Loss is a slight twist on something called the Likelihood Function. In fact, Log Loss is -1 * the log of the likelihood function. So, we will start by understanding the likelihood function.\n",
        "\n",
        "The likelihood function answers the question \"How likely did the model think the actually observed set of outcomes was.\" If that sounds confusing, an example should help.\n",
        "\n",
        "###Example\n",
        "A model predicts probabilities of [0.8, 0.4, 0.1] for three houses. The first two houses were sold, and the last one was not sold. So the actual outcomes could be represented numeically as [1, 1, 0].\n",
        "\n",
        "Let's step through these predictions one at a time to iteratively calculate the likelihood function.\n",
        "\n",
        "The first house sold, and the model said that was 80% likely. So, the likelihood function after looking at one prediction is 0.8.\n",
        "\n",
        "The second house sold, and the model said that was 40% likely. There is a rule of probability that the probability of multiple independent events is the product of their individual probabilities. So, we get the combined likelihood from the first two predictions by multiplying their associated probabilities. That is 0.8 * 0.4, which happens to be 0.32.\n",
        "\n",
        "Now we get to our third prediction. That home did not sell. The model said it was 10% likely to sell. That means it was 90% likely to not sell. So, the observed outcome of not selling was 90% likely according to the model. So, we multiply the previous result of 0.32 by 0.9.\n",
        "\n",
        "We could step through all of our predictions. Each time we'd find the probability associated with the outcome that actually occurred, and we'd multiply that by the previous result. That's the likelihood.\n",
        "\n",
        "###From Likelihood to Log Loss\n",
        "Each prediction is between 0 and 1. If you multiply enough numbers in this range, the result gets so small that computers can't keep track of it. So, as a clever computational trick, we instead keep track of the log of the Likelihood. This is in a range that's easy to keep track of. We multiply this by negative 1 to maintain a common convention that lower loss scores are better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU9qT-g2IhMW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e2d32abb-6f1d-4d45-daa7-4c66f9154458"
      },
      "source": [
        "from sklearn.metrics import log_loss\n",
        "log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],[[.1, .9], [.9, .1], [.8, .2], [.35, .65]])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.21616187468057912"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    }
  ]
}